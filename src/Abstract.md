The template that should be assumed is the staging layout of instruments, speakers, microphones, monitors, lighting/fogs, musicians positioned according to some stereo-imaging layout similar to how mixing and mastering works after recording sessions have taken place.

For example, if a band is playing then the lead vocal is at the middle of the stage, drums are placed behind everything with room and overhead mics dominating the bandwidths, two guitars are placed left and right while hard panned each output to their respective sides, bass is sidechained with drums as well as melody lines, backup vocals are spread acrosa the spectrum behing the lead vocal and slightly cutting the low-mid frequencies to keep the lead centered etc.


Now take all those ideas and conceptual frameworks into defining a computing architecture. Unlike traditional von-neumann architecture, we now have a dynamic environment with all music and sound engineering terms having their analogs in the programming domain.

For example, lead vocal -> main thread, backup vocals -> library functions and predefined subroutines, drums -> blocks of code quantized for iterative scheduling, microphone -> standard input function, speakers -> standard output terminals, monitor -> recursive/conditional/boolean, stereo-image -> compiled executable binaries, left-right panning -> simultaneous operator that runs executable code while retaining backward graph for reversing states, bassline -> low level processes that maintain kernel level tasks.
